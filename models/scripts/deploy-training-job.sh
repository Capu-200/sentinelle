#!/bin/bash
# Script de dÃ©ploiement de l'entraÃ®nement sur Cloud Run Jobs
# Usage: ./scripts/deploy-training-job.sh [PROJECT_ID] [JOB_NAME] [REGION] [VERSION]

set -e

PROJECT_ID=${1:-"sentinelle-485209"}
JOB_NAME=${2:-"sentinelle-training"}
REGION=${3:-"europe-west1"}
VERSION=${4:-"1.0.0"}

echo "ðŸš€ DÃ©ploiement de l'entraÃ®nement sur Cloud Run Jobs..."
echo "   Projet: $PROJECT_ID"
echo "   Job: $JOB_NAME"
echo "   RÃ©gion: $REGION"
echo "   Version: $VERSION"

# VÃ©rifier si le projet existe
if ! gcloud projects describe "$PROJECT_ID" &>/dev/null; then
    echo "âŒ Erreur: Le projet $PROJECT_ID n'existe pas ou vous n'avez pas les permissions"
    exit 1
fi

# DÃ©finir le projet actif
gcloud config set project "$PROJECT_ID"

# Activer les APIs nÃ©cessaires
echo "ðŸ”§ Activation des APIs..."
gcloud services enable run.googleapis.com \
    cloudbuild.googleapis.com \
    artifactregistry.googleapis.com \
    storage-api.googleapis.com \
    --project="$PROJECT_ID"

# Se dÃ©placer dans le dossier models
cd "$(dirname "$0")/.." || exit 1

# VÃ©rifier que les donnÃ©es existent
DATA_DIR="Data/processed"
if [ ! -d "$DATA_DIR" ]; then
    echo "âŒ Erreur: Le dossier $DATA_DIR n'existe pas"
    echo "   ExÃ©cutez d'abord: python scripts/clean_data.py"
    exit 1
fi

# CrÃ©er un Cloud Storage bucket pour les donnÃ©es (si n'existe pas)
BUCKET_NAME="${PROJECT_ID}-ml-data"
if ! gsutil ls -b "gs://$BUCKET_NAME" &>/dev/null 2>&1; then
    echo "ðŸ“¦ CrÃ©ation du bucket Cloud Storage..."
    gsutil mb -p "$PROJECT_ID" -l "$REGION" "gs://$BUCKET_NAME"
    echo "   âœ… Bucket crÃ©Ã©: gs://$BUCKET_NAME"
else
    echo "   âœ… Bucket existe dÃ©jÃ : gs://$BUCKET_NAME"
fi

# Uploader les donnÃ©es vers Cloud Storage
echo "ðŸ“¤ Upload des donnÃ©es vers Cloud Storage..."
echo "   Cela peut prendre quelques minutes..."
gsutil -m cp "$DATA_DIR"/*.csv "gs://$BUCKET_NAME/data/" 2>&1 | grep -E "(Copying|/)" || true
echo "   âœ… DonnÃ©es uploadÃ©es"

# CrÃ©er un .dockerignore pour exclure les gros fichiers
cat > .dockerignore << 'EOF'
Data/raw/*
Data/processed/*.csv
artifacts/*
__pycache__
*.pyc
.git
*.md
tests/
EOF

# DÃ©ployer le job Cloud Run avec le Dockerfile.training
echo "ðŸ“¦ Construction et dÃ©ploiement du job..."
echo "   Cela peut prendre 5-10 minutes (premiÃ¨re fois)..."

# Renommer temporairement Dockerfile.training en Dockerfile pour le build
if [ -f "Dockerfile.training" ]; then
    mv Dockerfile Dockerfile.api 2>/dev/null || true
    cp Dockerfile.training Dockerfile
fi

gcloud run jobs deploy "$JOB_NAME" \
    --source . \
    --region="$REGION" \
    --set-env-vars="DATA_DIR=/app/data,ARTIFACTS_DIR=/app/artifacts,BUCKET_NAME=$BUCKET_NAME,VERSION=$VERSION" \
    --memory=16Gi \
    --cpu=8 \
    --task-timeout=7200 \
    --max-retries=1 \
    --project="$PROJECT_ID" \
    --quiet

# Restaurer le Dockerfile original
if [ -f "Dockerfile.api" ]; then
    mv Dockerfile.api Dockerfile
fi

echo ""
echo "âœ… Job crÃ©Ã©!"
echo ""
echo "ðŸ“‹ Pour lancer l'entraÃ®nement:"
echo "   gcloud run jobs execute $JOB_NAME --region=$REGION --project=$PROJECT_ID"
echo ""
echo "ðŸ“Š Pour suivre les logs en temps rÃ©el:"
echo "   gcloud run jobs executions list --job=$JOB_NAME --region=$REGION --project=$PROJECT_ID --limit=1"
echo "   gcloud run jobs executions logs read <EXECUTION_NAME> --region=$REGION --project=$PROJECT_ID"
echo ""
echo "ðŸ’¾ Les artefacts seront sauvegardÃ©s dans:"
echo "   gs://$BUCKET_NAME/artifacts/"

