"""
Script principal d'entraÃ®nement du pipeline ML complet.

Orchestre :
1. PrÃ©paration (split temporel)
2. Feature engineering
3. EntraÃ®nement supervisÃ©
4. EntraÃ®nement non supervisÃ©
5. Calibration des seuils
6. Sauvegarde des artefacts
"""

from __future__ import annotations

import argparse
import json
import os
import pickle
import sys
from pathlib import Path

import pandas as pd

# Ajouter le rÃ©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.preparation import prepare_training_data
from src.features.training import compute_features_for_dataset
from src.models.supervised.train import train_supervised_model
from src.models.unsupervised.train import train_unsupervised_model
from src.utils.versioning import save_artifacts


def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(description="EntraÃ®nement du pipeline ML")
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path(os.getenv("DATA_DIR", "Data/processed")),
        help="Dossier contenant les donnÃ©es nettoyÃ©es (ou variable DATA_DIR)",
    )
    parser.add_argument(
        "--config-dir",
        type=Path,
        default=Path("configs"),
        help="Dossier contenant les configurations",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=Path(os.getenv("ARTIFACTS_DIR", "artifacts")),
        help="Dossier oÃ¹ sauvegarder les artefacts (ou variable ARTIFACTS_DIR)",
    )
    parser.add_argument(
        "--version",
        type=str,
        default="1.0.0",
        help="Version du modÃ¨le (SemVer)",
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Mode local: utilise tous les cores et dataset complet (pas d'Ã©chantillonnage)",
    )
    parser.add_argument(
        "--train-split-date",
        type=str,
        help="Date de fin du set d'entraÃ®nement (ISO format)",
    )
    parser.add_argument(
        "--val-split-date",
        type=str,
        help="Date de fin du set de validation (ISO format)",
    )

    args = parser.parse_args()

    print(f"ğŸš€ DÃ©marrage de l'entraÃ®nement (version {args.version})")
    print(f"ğŸ“ DonnÃ©es : {args.data_dir}")
    print(f"âš™ï¸  Config : {args.config_dir}")
    print(f"ğŸ’¾ Artefacts : {args.artifacts_dir}")
    print()

    # ========== 1. PRÃ‰PARATION DES DONNÃ‰ES ==========
    print("=" * 60)
    print("Ã‰TAPE 1: PrÃ©paration des donnÃ©es")
    print("=" * 60)
    
    # Dataset PaySim (supervisÃ©)
    paysim_path = args.data_dir / "paysim_mapped.csv"
    if not paysim_path.exists():
        raise FileNotFoundError(f"Dataset PaySim non trouvÃ©: {paysim_path}")
    
    print(f"ğŸ“Š Chargement PaySim: {paysim_path}")
    paysim_df = pd.read_csv(paysim_path)
    paysim_df["created_at"] = pd.to_datetime(paysim_df["created_at"], utc=True)
    print(f"   âœ… {len(paysim_df)} transactions chargÃ©es")
    
    # Split temporel PaySim
    print(f"\nğŸ“Š Split temporel PaySim...")
    paysim_train, paysim_val, paysim_test = prepare_training_data(
        paysim_path,
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
    )
    
    # Dataset Payon Legit (non supervisÃ©)
    payon_path = args.data_dir / "payon_legit_clean.csv"
    if not payon_path.exists():
        raise FileNotFoundError(f"Dataset Payon non trouvÃ©: {payon_path}")
    
    print(f"\nğŸ“Š Chargement Payon Legit: {payon_path}")
    payon_df = pd.read_csv(payon_path)
    payon_df["created_at"] = pd.to_datetime(payon_df["created_at"], utc=True)
    print(f"   âœ… {len(payon_df)} transactions chargÃ©es")
    
    # Split temporel Payon
    print(f"\nğŸ“Š Split temporel Payon...")
    payon_train, payon_val, payon_test = prepare_training_data(
        payon_path,
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
    )
    
    # ========== 2. FEATURE ENGINEERING ==========
    print("\n" + "=" * 60)
    print("Ã‰TAPE 2: Feature Engineering")
    print("=" * 60)
    
    # DÃ©terminer le nombre de jobs
    import multiprocessing as mp
    n_cores = mp.cpu_count()
    
    if args.local:
        # Mode local: utiliser tous les cores disponibles (optimisÃ© pour 10 cores / 32GB RAM)
        n_jobs = max(1, n_cores - 1)  # Laisser 1 core libre
        use_full_dataset = True
        print(f"\nâš™ï¸  Configuration LOCAL: {n_jobs} processus parallÃ¨les (sur {n_cores} cores)")
        print(f"   ğŸ’¡ Mode local: dataset complet, pas d'Ã©chantillonnage")
    else:
        # Mode Cloud: rÃ©duire pour Ã©viter OOM
        n_jobs = min(5, max(1, n_cores - 2))  # Max 5 processus, laisser 2 cores libres
        use_full_dataset = False
        print(f"\nâš™ï¸  Configuration CLOUD: {n_jobs} processus parallÃ¨les (sur {n_cores} cores)")
        print(f"   ğŸ’¡ Mode Cloud: Ã©chantillonnage activÃ© pour Ã©viter timeout")
    
    # Features pour PaySim (supervisÃ©)
    if use_full_dataset:
        # Mode local: utiliser le dataset complet
        paysim_train_sample = paysim_train
        print(f"\nğŸ”§ Calcul des features PaySim (train) - DATASET COMPLET...")
        print(f"   ğŸ“Š {len(paysim_train_sample):,} transactions (dataset complet)")
    else:
        # Mode Cloud: Ã©chantillonnage pour accÃ©lÃ©rer
        paysim_train_sample = paysim_train.sample(
            n=min(500000, len(paysim_train)),
            random_state=42
        ).sort_values("created_at").reset_index(drop=True)
        print(f"\nğŸ”§ Calcul des features PaySim (train)...")
        print(f"   âš ï¸  Ã‰chantillon: {len(paysim_train_sample):,} transactions (sur {len(paysim_train):,})")
        print(f"   ğŸ’¡ Pour l'entraÃ®nement complet, utiliser --local")
    
    paysim_train_features = compute_features_for_dataset(
        paysim_train_sample,
        verbose=True,
        n_jobs=n_jobs,
        chunk_size=1000,  # Chunks de 1000 transactions pour Ã©viter la surcharge mÃ©moire
    )
    paysim_train_labels = paysim_train_sample["is_fraud"] if "is_fraud" in paysim_train.columns else None
    
    print(f"\nğŸ”§ Calcul des features PaySim (val)...")
    paysim_val_features = compute_features_for_dataset(
        paysim_val,
        verbose=True,
        n_jobs=n_jobs,
        chunk_size=1000,
    )
    paysim_val_labels = paysim_val["is_fraud"] if "is_fraud" in paysim_val.columns else None
    
    # Features pour Payon (non supervisÃ©)
    print(f"\nğŸ”§ Calcul des features Payon (train)...")
    payon_train_features = compute_features_for_dataset(
        payon_train,
        verbose=True,
        n_jobs=n_jobs,
        chunk_size=1000,
    )
    
    print(f"\nğŸ”§ Calcul des features Payon (val)...")
    payon_val_features = compute_features_for_dataset(
        payon_val,
        verbose=True,
        n_jobs=n_jobs,
        chunk_size=1000,
    )
    
    print(f"\nâœ… Features calculÃ©es:")
    print(f"   PaySim train: {len(paysim_train_features)} transactions, {len(paysim_train_features.columns)} features")
    print(f"   PaySim val: {len(paysim_val_features)} transactions, {len(paysim_val_features.columns)} features")
    print(f"   Payon train: {len(payon_train_features)} transactions, {len(payon_train_features.columns)} features")
    print(f"   Payon val: {len(payon_val_features)} transactions, {len(payon_val_features.columns)} features")
    
    # ========== 3. ENTRAÃNEMENT SUPERVISÃ‰ ==========
    print("\n" + "=" * 60)
    print("Ã‰TAPE 3: EntraÃ®nement ModÃ¨le SupervisÃ© (LightGBM)")
    print("=" * 60)
    
    if paysim_train_labels is None:
        print("âš ï¸  Pas de labels dans PaySim, skip entraÃ®nement supervisÃ©")
        supervised_model = None
    else:
        print(f"ğŸ“Š EntraÃ®nement sur {len(paysim_train_features)} transactions")
        print(f"   Fraudes: {paysim_train_labels.sum()} ({paysim_train_labels.mean()*100:.2f}%)")
        
        supervised_model = train_supervised_model(
            train_data=paysim_train_features,
            train_labels=paysim_train_labels,
            val_data=paysim_val_features,
            val_labels=paysim_val_labels,
        )
        
        print(f"âœ… ModÃ¨le supervisÃ© entraÃ®nÃ©")
    
    # ========== 4. ENTRAÃNEMENT NON SUPERVISÃ‰ ==========
    print("\n" + "=" * 60)
    print("Ã‰TAPE 4: EntraÃ®nement ModÃ¨le Non SupervisÃ© (IsolationForest)")
    print("=" * 60)
    
    print(f"ğŸ“Š EntraÃ®nement sur {len(payon_train_features)} transactions (normales uniquement)")
    
    unsupervised_model = train_unsupervised_model(
        train_data=payon_train_features,
    )
    
    print(f"âœ… ModÃ¨le non supervisÃ© entraÃ®nÃ©")
    
    # ========== 5. CALIBRATION DES SEUILS ==========
    print("\n" + "=" * 60)
    print("Ã‰TAPE 5: Calibration des Seuils")
    print("=" * 60)
    
    # Calculer les scores sur le validation set
    if supervised_model and paysim_val_labels is not None:
        supervised_scores = supervised_model.predict(paysim_val_features)
    else:
        supervised_scores = None
    
    unsupervised_scores = unsupervised_model.predict(payon_val_features)
    
    # Calculer les seuils (top 0.1% BLOCK, top 1% REVIEW)
    if supervised_scores is not None:
        # Utiliser le score supervisÃ© pour les seuils
        block_threshold = supervised_scores.quantile(0.999)  # Top 0.1%
        review_threshold = supervised_scores.quantile(0.99)  # Top 1%
    else:
        # Utiliser le score non supervisÃ©
        block_threshold = unsupervised_scores.quantile(0.999)
        review_threshold = unsupervised_scores.quantile(0.99)
    
    thresholds = {
        "block_threshold": float(block_threshold),
        "review_threshold": float(review_threshold),
    }
    
    print(f"âœ… Seuils calculÃ©s:")
    print(f"   BLOCK threshold: {block_threshold:.4f}")
    print(f"   REVIEW threshold: {review_threshold:.4f}")
    
    # ========== 6. SAUVEGARDE DES ARTEFACTS ==========
    print("\n" + "=" * 60)
    print("Ã‰TAPE 6: Sauvegarde des Artefacts")
    print("=" * 60)
    
    # CrÃ©er le dossier de version
    version_dir = args.artifacts_dir / f"v{args.version}"
    version_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarder les modÃ¨les
    if supervised_model:
        supervised_path = version_dir / "supervised_model.pkl"
        supervised_model.save(supervised_path)
        print(f"âœ… ModÃ¨le supervisÃ© sauvegardÃ©: {supervised_path}")
    
    unsupervised_path = version_dir / "unsupervised_model.pkl"
    unsupervised_model.save(unsupervised_path)
    print(f"âœ… ModÃ¨le non supervisÃ© sauvegardÃ©: {unsupervised_path}")
    
    # Sauvegarder les seuils
    thresholds_path = version_dir / "thresholds.json"
    with open(thresholds_path, "w") as f:
        json.dump(thresholds, f, indent=2)
    print(f"âœ… Seuils sauvegardÃ©s: {thresholds_path}")
    
    # Sauvegarder le schÃ©ma de features (liste des colonnes)
    feature_schema = {
        "version": args.version,
        "features": list(paysim_train_features.columns) if supervised_model else list(payon_train_features.columns),
    }
    schema_path = version_dir / "feature_schema.json"
    with open(schema_path, "w") as f:
        json.dump(feature_schema, f, indent=2)
    print(f"âœ… SchÃ©ma de features sauvegardÃ©: {schema_path}")
    
    # CrÃ©er/mettre Ã  jour le symlink latest
    latest_path = args.artifacts_dir / "latest"
    if latest_path.exists():
        latest_path.unlink()
    latest_path.symlink_to(f"v{args.version}")
    print(f"âœ… Symlink 'latest' â†’ v{args.version}")
    
    # ========== RÃ‰SUMÃ‰ ==========
    print("\n" + "=" * 60)
    print("âœ… ENTRAÃNEMENT TERMINÃ‰")
    print("=" * 60)
    print(f"Version: {args.version}")
    print(f"Artefacts: {version_dir}")
    print(f"\nModÃ¨les entraÃ®nÃ©s:")
    if supervised_model:
        print(f"  âœ… SupervisÃ© (LightGBM)")
    print(f"  âœ… Non supervisÃ© (IsolationForest)")
    print(f"\nSeuils:")
    print(f"  BLOCK: {block_threshold:.4f}")
    print(f"  REVIEW: {review_threshold:.4f}")


if __name__ == "__main__":
    main()
